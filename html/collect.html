<html><head>
 <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Inconsolata">
 <link rel="stylesheet" href="../css/style.css" type="text/css" media="screen">
</head>
<body><pre>
20150930T1422 another possible definition of redo ^
The definition for ^ given yesterday were as follows:

S successor

f:unary
n:numeral
y:numeral
f^0 y gives y
f^(S n) y gives f^n f y

f:binary
g:binary
x:numeral
y:numeral
0 f^g y gives y
x f^g y gives (x g y) f^g (x f y)

Today I present an alternate form of evaluation in an attempt to make the closets connection to the classical form of recursion that most are familiar with.
Anytime I say recursion I mean primitive recursion unless otherwise noted for general recursion allows methods of calculation which rely on logical proofs in certain logical systems where as my goal is to follow Goodstein by sticking to arithmetic first and deriving logical operations as a consequence of arithmetic.
Due to the nature of recursion as revealed by Peter in her Recursive Functions, many forms of recursive definition are reducible to primitive recursion in one argument without parameters.
For example, course of values recursion is eliminable to primitive recursion in a single variable without parameters via the fundamental theorem of arithmetic.
Finding the correct form of definition for f^g means giving one which hints, as much as possible, to practical forms of these powerful reductions.

The form of definition today is derived more directly from the definition by recursion as given by Goodstein on pg.19 of RNT:

"
 F(x,0)=a(x)
F(x,Sy)=b(x,y,F(x,y))
" Goodstein RNT pg. 19

I've given his scheme for definition by recursion in his notation.
In N one would write these expressions as:

  (0 F y) ~ a y
(S.x F y) ~ (x F y) b x,y

or

    0.F.y ~ a.y
(S.x).F.y ~ x.F.y b x,y

I prefer the former to the latter for now.
First, the reason the recursion in N is written so that it is a recursion in x rather than in Goodstein's definition where it is recursion in y.
In N, the convention is that x is used most often to refer to the left argument to a dyadic verb, where as y usually refers to the right argument of a dyadic or monadic verb.
A further convention which has far reaching implications for the way in which we think about recursively defined verbs is the use of the left argument as the argument in which the recursion is in.
The reason is because in order to better express the functional relations needed to successfully retain analogs to classical analytic results when using only primitive recursive rational functions it becomes paramount to imagine a family of functions index by a natural numeral.
In N an expression such as 3.f or (3 f) gives a projection of a dyadic (binary) function (verb) f whose left argument is 3.
For example

 f:+     f is plus
 3.f     3 of f
3+       3 plus
 g:3.f   g is 3.f
 g 4
7
 3+ 4
7

Many functions in primitive recursive analysis are used via similar forms in majorizing arguments which play the role of classical limit arguments.
The relatively exponential function E is defined recursively from the power function (pow) and the factorial function (fac) so that

n numeral
y rational

  (0 E y) ~ 1
(S.n E y) ~ (n E y) + (y pow S.n) % fac S.n

So that for a given problem, one may need only 3.E or perhaps 5.E or n.E for some n to carry through an argument or computation.
The advantage being not only all the machinery of primitive recursive analysis, but immediate computability for experiment or application.

Upon reviewing the definition I gave yesterday I realized its symmetry was not properly reflective of the form of recursive definition more commonly used.

An alternate way of interpreting f^g for a pair of dyadic (binary) verbs f and g is as follows

  (0 f^g y) ~ y
(S.x f^g y) ~ (x f^g y) f x g y

As is easily seen, this more closely reflects the statement of Goodstein's schema for definition by primitive recursion given above (and reproduced her for immediate comparison:

      0.F.y ~ a.y
  (S.x).F.y ~ x.F.y b x,y

  (0 f^g y) ~ y
(S.x f^g y) ~ (x f^g y) f x g y

Or, perhaps to better complete the established pattern to the base case:

  (0 f^g y) ~ 0 g y
(S.x f^g y) ~ (x f^g y) g x f y

This suggests that the unary form of f^g be defined via the following substitution into its binary counterpart:

(f^g y) ~ y f^g y

So that the factorial function is defined as:

fac: *^{1+x}
fac 3
*^{1+x} 3
3 *^{1+x} 3
(2 *^{1+x} 3) * 2{1+x}3
(2 *^{1+x} 3) * 1+2
(2 *^{1+x} 3) * 3
(1 *^{1+x} 3) * (1{1+x}3) * 3
(1 *^{1+x} 3) * (1+1) * 3
(1 *^{1+x} 3) * 2 * 3
(0 *^{1+x} 3) * (0{1+x}3) * 2 * 3
(0 *^{1+x} 3) * (1+0) * 2 * 3
(0 *^{1+x} 3) * 1 * 2 * 3
(0{1+x}3) * 1 * 2 * 3
(1+0) * 1 * 2 * 3
1 * 1 * 2 * 3
1 * 1 * 6
1 * 6
6



20150929T1522 redo ^
First, ^ is referred to as “redo” or “repeat” it is the entry point for “recursion” into N notation.
For a numeral n and unary function f

f^0 y gives y
f^(S n) y gives  f^n f y

In other words, it’s just iteration (where S is the successor operation).
Now here’s where things get fun, and perhaps profoundly interesting.
The generalization of the operator ^ for two binary functions f and g is as follows

0 f^g y gives y
x f^g y gives (x g y) f^g (x f y)

Thus if P is the predecessor operation then

(S n) f.{y}^P.{x} y gives n f.{y}^P.{x} f y (after computation where {y} is projection of the right argument and {x} is projection of the left argument).

Which is the same as simple iteration in the form (f^n y)

If f and g are both unary we can make the agreement that

0 f^g y gives y
x f^g y gives g.x f^g f.y

Which lets us write

n S^P.P 0

for the floor of half of n (or the integer half of n).

A function whose recursive definition is classically given via a helper function:

alt 0 gives 0
alt S n gives 1 - alt n  (here - is monus i.e. m - 0 gives m and m - S n gives P m-n)

so that alt 0 gives 0, alt 1 gives 1, alt 2 gives 0 and so on.

then

hf 0 gives 0
hf S n gives (hf n) + alt n

where hf is the the integer half function.

Here is probably a more “likable” definition

n *^P 1

is the factorial of n (where P is the predecessor and  * is normal multiplication).
The following sequence of events showcases how you might imagine working with this notation:

4 *^P 1
P.4 *^P 4*1
3 *^P 4
P.3 *^P 3*4
2 *^P 3*4
P.2 *^P 2*3*4
1 *^P 2*3*4
P.1 *^P 1*2*3*4
0 *^P 1*2*3*4
1*2*3*4
24

Similarly

n +^P 0

is the sum of the numbers from 1 to n.
The importance is that these are recursive definitions.
One can use a different notation, the unary numer function !, to achieve the same things

!2 gives (0,1)
!3 gives (0,1,2)
!10 gives (0,1,2,3,4,5,6,7,8,9) 
and so on

so that the factorial of n can be given as

*/1+!n

which is times over numer n.

*/1+!3
*/1+(0,1,2)
*/(1,2,3)
1*2*3
6

Which is a more “explicit” method of calculating with factorials (this list the way factorial is usually given in text books when people write (fact n) = 1 * 2 * 3 * 4 * … *n

(x*y) is the same as x+^y 0
(x exp y) is the same as x*^y 1


20150929T1251 Refining the primitive concepts of N
The greatest contribution to my development of N has been identifying the dyadic adverb ^ as redo|again|repeat.
I've settled, for now, on redo because it is appropriately vague on how you will redo the next step, perhaps with some minor edits or changes (which is what happens in most places where it might be used).
Though, in a barely second place to ^ as redo, are the following definitions/identifications:

&gt; max
= pd
&lt; min
&amp; gcd|meet
| lcm|join
~ same

Though, as will hopefully become clearer as I develop N and my presentation of its utility, the identification of append|affix|concat which is denoted , as a binary operation (dyad) should have the greatest impact on how mathematicians view classical expressions such as f(x,y,z) (mostly because this notation maintains its classical meaning in N's new interpretation).

I'll give a brief explanation of what = means (it's not that complicated really).

 2=1
1
 5=9
4
 9=5
4
 105=99
6

The dyadic verb = is called "positive difference" and is the most natural "norm" on the natural numbers.
My contention is that the sign = should no longer be used to denoted equality because that concept is poorly defined in most cases.
Specifically, = should be included among the arithmetical signs and not among logical signs.
That is where ~ comes in, as same, it is much more appropriate, in general, for expressing the similarities that we are actually used to seeing in algebraic expressions.

Furthermore, there is a general outlook on arithmetic which is very clearly and exactly described in Goodstein's Recursive Number Theory that supports the use of = as positive difference.
Specifically, it is important to prove or "know" that if the positive difference between one numeral and another is zero then they must be the same numeral.
This has far reaching implications in how we conceive of sameness inside a theory and outside a theory.
In other words, it is important to know where arithmetic ends and where our statements about arithmetic begin.
Though it might sound like a theoretical issue only, it is not.
Ask any computer scientist and you will see that the notion of equality is usually a matter of "relevant taste".
I've decided to subsume equality as a concept under the moniker "same" for now, though it might stick as I go along.

In the other notation of N the positive difference satisfies the following identity:

{x=y} ~ {(x-y)+y-x}

Though this is given in the curly bracket notation (something which I'm still trying to decide whether it helps thinking or if its just a redundant crutch).

To put into symbols the statement that two numerals whose positive difference is zero are the same we write:

(0 ~ x = y) ~ x ~ y

To some people, this might be profound (because it is) and to others it might seem silly because of how simple it is (because it is).

Why make &lt; and &gt; return min and max respectively?
There are at least two arguments: one is "practical" the other is theoretical|design based.
The practical argument is that deciding whether one numeral is greater than or less than another is a statement about numerals, where as giving the the maximum of a pair of numerals is an arithmetic action.
Another practical observation is that no matter where you might be "incrementing or decrementing" (something that you shouldn't be doing with N in the first place, but that's something completely different to write about) you can use the this claw *= to do what you've really been wanting.
For example

 1 *= 3
1
 2 *= 3
1
 3 *= 3
0
 5 *= 1
1
 4 *= 2
1
 3 *= 3
0

There is also the situation that frequently someone wanting to "compare" numbers are doing so because they desire to find the minimum or maximum of them.
It's different, but it's not different to be different.
It's different because it's something that cuts closer to the "heart" of arithmetic and its relation to statements about arithmetic.
This being the theory argument (the details of the theory are easily read in Goodstein RNT, but because people are afraid of thinking I've not gone into detail here so as not to scare them off).
Suppose you wish to decide whether one number is greater than another.
Supposing you believe 0 to represent "True" and 1 to represent "False" you could write

less:{x=x&lt;y}
 3 less 4
0
 4 less 3
1


Suppose you believe 0 to represent "False" and 1 to represent "True" you could write

less:{1-x=x&lt;y}
 3 less 4
1
 4 less 3
0

"Why do this!" you ask!!?
Because, orderings and decision is actually hard in general.
It is highly dependent on representation.
Under certain conditions you may find it necessary to redefine what it means for things to be less than or greater than: even when it comes to something so primitive as the natural numbers!
An example, but perhaps not the most accessible example, is the modular representation of integers described by Knuth in TAOCP Vol 2 Semi-Numerical Algorithms.
"But! Giving the max requires you to make a decision as to which is greater!"
No, it doesn't: consult the identity that follows.

(x &gt; y) ~ x+y-x
(x &gt; y) ~ y+x-y

Remember, in N, the dyad - is monus, not minus.
Minus is actually a powerful abstraction, where as monus is a familiar activity (because you can't take an apple from an empty bag and produce a negative apple).
The behavior of - on integer numerals is the operation of minus that everyone is happy being familiar with (for now).

The theoretical reason to considering = &lt; &gt; as pd, min, max is because these arithmetic operations are prior to the relations of equality, less than, and greater than.
It is a subtle but essential distinction, one which is bound to frustrate and baffle those who are not willing to entertain such a fundamental change in perspective.

There is an even deeper reason for defining things this way.
Propositions about arithmetic and arithmetic should be separated as much as possible so that it is easier to interpret propositions about arithmetic in arithmetic in ways that we have yet to imagine.
For example, there are different ways we can think of "ordering" the natural numbers.
Perhaps we want to imagine all the even numbers coming before the odd numbers!
Our imagination in this regard could change on a whim, but by agreeing on x&lt;y as an abbreviation for x+y-x or y+x-y we are focusing on how the fundamental arithmetic operations of plus and monus combine with each other.
In other words, whatever it is that we might wish from an arithmetic, we seem to be bound to introduce the concept of plus and monus from which the expression

(x+y-x) ~ y+x-y

is satisfied by any numerals x and y.
The fact that I have called &lt; min and &gt; max already "plays favorites" to the classical ordering of the natural numerals, but this is a design feature so that users are not immediately confused.

Another way of thinking about these thing is to see that the classical notion that a numeral x is less than or equal to a numeral y is encapsulated in the following proposition:

x ~ y-y-x

Again, this might seem silly, but, much like the makers of the C programming language said, it wears well as one uses it.
It opens doors from the beginning without overwhelming the user of mathematics with unnecessary or overwhelming choices.
One might think of it as a more "neutral" way of doing arithmetic.
A erudite would call this perspective "Post-Gödelian Arithmetic".
The reason?
All of these seemingly annoying redefining of age old concepts bare out all the way to the limits of arithmetic.
Specifically, to the point where arithmetic can be used to say that "There is an equation 0 ~ f n which is verifiable but not provable in our primitive arithmetic".
This seemingly abstract fact is actually considerably concrete, and the arithmetic conventions of N take this concrete fact into account from the beginning.
This fact follows form a specific interpretation of propositions about arithmetic as parts of arithmetic (Gödel Numerals).



20150927T1517 Aren't You Worried about someone stealing your ideas?
Yes and no.
I am worried about people not giving credit where credit is due e.g. Goodstein's work is largely overlooked even though he, like Russell, Hilbert, and others, has produced a monumentally important contribution to the foundation of mathematics and mathematical philosophy.
If someone copies my ideas then at least there will be detailed and public records that my work has been done by me over an iterated period of time.


20150927T1512 Primitive Inequalities
Any inequalities from Hardy, Littlewood, and Pólya Inequalities which are provable or derivable inside Goodstein's Primitive Recursive Arithmetic are to be called Primitive Inequalities.
Though there is the suggestion that they should be called elementary, there are reasons for not favoring this phrase which clash with the use of elementary to describe a class of functions.
It is important to identify which inequalities from Hardy, Littlewood, and Pólya Inequalities are provable in Goodstein's system as they are the principle relations used to establish what are commonly accepted as essential analytic results.
Comparing where these inequalities are used in Goodstein's Recursive Analysis will reveal the different branches of analysis that are united and divided by Goodstein's methods.


20150927T1451 The Design of Distributed and Parallel Computing Systems
We lack a practical foundation for parallel and distributed computing systems.
This is not because we lack the knowledge needed to understand these topics, rather we lack the proper narrative to carry this knowledge to those who need it.
Those who need it is a subjective group, but, it is my contention, that elementary school students need such knowledge.
They don't need knowledge of parallel computing, but they need the notation to describe their naturally developing tabular thought.
Humans think recursively and act iteratively.
Iterative actions can be parallel or "distributed" (not exclusively).
The elementary school use of phrases like "do the same thing again" or "repeat that but now on this one" is just one of the ways in which we introduce parallelism (or perhaps just concurrency) into the minds of elementary school students.
Multidimensional arithmetic is just one access point for putting parallel concepts into the hands of those who need it most.


20150927T1440 Goodstein and Iverson and Peter
I've written much on how the works of Goodstein and Iverson just work together.
There are some hints, though nothing explicit, that Iverson was aware of Goodstein's work but did not care to mention it.
Specifically, the writings in Goodstein's "Fundamental Concepts of Mathematics" are such that Iverson would have been not only attracted to its content but also its conceptual perspective.
One thing which is striking is that Goodstein's FCOM was published in 1962 and Iverson's "A Programming Language" was published in that same year.
In Goodstein's FCOM he uses the phrase "pronumerals" which is also used by Iverson in APL.
I could not find a single mention in Iverson or Goodstein of the others work.
I continue to find statements of having "making computation a mathematical activity" in papers and publications without seeing any mention of the works of Peter or Goodstein.
Goodstein makes it clear, multiple times throughout his work, that his efforts are built entirely on those of Peter and one can even find the appropriate references to the Arithmetization of Logic given by Kleene in his Introduction to Metamathematics.
It is very frustrating to me to not see more mention of Peter's work on Recursive Functions in modern literature.
This is not only because she is a woman, something which people tend to overlook or forget to mention, but because before her work on recursive functions there was no single collection of what constitute recursive functions and how the differing forms of recursion and recursiveness are or are not reducible to forms of primitive recursion.

As much as modern computation is about recursion, Peter should be more frequently mentioned in modern texts because it is because of her that we have most of our fundamental results in the foundation of recursion and recursiveness.


20150927T1138 Goodstein Realized Leibniz' Calculus ratiocinator
I am surprised that more was not made of Goodstein's work at the time it was completed.
For those familiar with Leibniz's efforts to develop what he referred to as a "Calculus Ratiocinator" they will find in Goodstein's 'Recursive Number Theory' a complete exposition of the ideal desired by Leibniz.
Sadly, due to the popularity of Leibniz's work, there has been a great number of misconceptions as to what was or would become his final aim in creating a Calculus ratiocinator.
Some would describe his desired goal as an "algebra of rational thought" or as an anticipation of modern mathematical logic.
Leibniz's contributions to modern mathematical logic and philosophy are perhaps best encapsulated in Russell's work, though certainly there is no substitute for the primary sources.

To answer the question "In what way did Goodstein realize Leibniz's Calculus Ratiocinator?".
Goodstein's recursive number theory is a surprisingly simple formalization of primitive recursive function theory, but, more importantly, it is an intuitive foundation for all of number theory.
Furthermore, he is able to show how what passes as modern mathematical logic is developed entirely through the use of arithmetic operations.
It is important to contrast this with the works of Kleene and Gödel.
Kleene and Gödel both developed an arithmetization of logic as a tool for reducing their metamathematical arguments to the most "trustworthy" operations of arithmetic.
Goodstein began not by seeking out a logical machinery, but rather a self contained description of arithmetic as the art of primitive recursive reductions and abbreviations.

It is known that Leibniz attempted many times to develop his Calculus Ratiocinator using the elementary operations of arithmetic.
One hardly needs a primary source to imagine a man of his intellect trying to seek out patterns in arithmetic which matched or met his desired goals in research.
Prior to any of our modern mathematical logic was elementary arithmetic, and it is in elementary arithmetic that notions of system, structure, and irrefutable proof and truth developed.
A modern mathematician would say "Leibniz sought a model of mathematical logic in elementary arithmetic" though it took the works of Boole to give a clearer hint at where one might find an arithmetic or algebra of argument.

Leibniz would have recognized immediately that Goodstein's formalism of primitive recursive arithmetic is precisely the Calculus ratiocinator sought for the following reasons:
1 The only inference rules are rules of substitution which encapsulate his 'identity of indiscernibles' and primitive recursive uniqueness rule which is a further application of identity of indiscernibles.
2 All arguments are made by eliminating notation until a numeral is reached.
3 Goodstein's development of mathematical logic in his primitive recursive arithmetic satisfies Leibniz's law of identity/contradiction.
4 In Goodstein's system only verifiable equations are provable which I believe Leibniz would have quickly interpreted as an interpretation of his law of Sufficient Reason.
5 Perhaps the most important thing, Goodstein has produce a clear, complete, simple, and general specification of the mechanical movements need by any notation which wishes to capture the most basic of mathematical acts: elementary arithmetic.

The reasons for believing Goodstein's work realizes Leibniz's ideal go on and on.
Anyone familiar with Leibniz should read Goodstein and see what they've missed all these years.
I can not overstate the importance of making this connection between the work of Leibniz and the work of Goodstein.
It may come as no surprise that my work on N has been inspired and motivated by a desire to craft these concepts in the most computable and calculable form possible.
Modern mathematics, while a tool of great power and generality, is still a poorly designed tool, one whose use requires an almost comical amount of expertise to wield wisely.
My purpose in making N and in my work is to refine the design of mathematics so that its use may give the widest impact on everyday life which it inevitably must have.


20150926T1631 Goodstein's Rt is a number theorists isqrt 
The integer square root of a number n is the largest number whose square is less than or equal to n.
It is possible to restrict this to just less than, and since I have not considered the boundary cases yet I will go with the convention established by number theorists.
The integer square root is fundamental to modern mathematical behavior.
It is also an essential part in the reduction proofs created by Peter in her Recursive Functions.
Its importance across all mathematics is not yet widely appreciated because its explicit use is often not mentioned.
It's importance is so significant that I have dedicated the unary symbol % of N to the isqrt operation.
From now on % will, in my mind, denote the integer square root of its argument in unary, and will denote the quotient of x divided by y in the binary case.

20150925T1520 Topics to Think About
Growing Ideas and Understanding with Git and GitHub
Emergence of logic from arithmetic rather than the other way around.
 Logic as a tool for thought is powerful but requires expert experience.
 Arithmetic as a tool for thought is available to all, and as a foundation for future understanding has yet to be fully utilized.


20150925T1455 Things I Should Think More About and Do
The design of this website is ugly.
This is primarily because I have put my time and effort into learning and understanding topics than I have into presenting what I've learned and understood about those topics.
There is a balance that one must strike in the work that they do.
That is a balance between doing work and adequately presenting the work that you have done.
When we say "presenting the work we've done" we're actually completing the final step of solving a problem: look back.
When we look back at what we've done we see how to change and alter its presentation so that whatever solution we've found might be seen at-a-glance or at least may be processed by our future self or others with the least amount of cognitive effort.

Another reason the design this website is ugly is because you can not design around nothing.
Before I spend time on a design I must identify relevant constraints on that design.
For this website the primary constraint is the content that I choose to put here.
Without clarifying the content I can not begin to clarify a design.
Another way of saying this is that if design has a goal then that goal must be identified before we begin the design process.
Here the goal is likely something like "Present arguments and information with crystal clarity."
This means that there should be as few moving parts to the mechanism of presentation as possible for the arguments and information given here already have an over abundance of moving parts.


20150925T1454 
The things people see and feel are guided by their thoughts and impressions for better or for worse.


20150924T1655 Dyad # take|project? Dyad , inject|affix|append
These verbs are obviously some of the most important in N regardless of what their ultimate implementation will be, the conceptual kernel of each must be so unavoidably necessary that one could not imagine living without them.
For # it's operation is projection i.e. it is what a mathematician would most likely call a pi-function (not the number pi, the function pi).
It's left argument is a list of indexes and its right is the the object to be projected.
Since N supports something like python's tuples, or C++'s homogenous vectors, it is easy to see that projection is what is happening if you take the items at the left arguments indices.
What's hard to know is if take is the correctly word for humans to use when referring to these general acts of projection, especially when dealign with tree like objects, dictionary|map like objects, or other objects.
I'm not saying that anything like dictionary|map is unavoidable as an object(noun), but that one must consider whether take is an appropriate verb for such things over project in case they are ultimately found to be unavoidable nouns for achieving N's goals.
x # y when x is a numeral and y is a numeral vector (table?) returns x items from y going back to the beginning of y if x exceeds the index of y i.e. the k-th item of x # y is the item of y whose index is the remainder of the length of y divided by k.
This style of index arithmetic is intended to eliminate references to items outside the index of a noun.
Some see this as a huge loss of opportunity to collect common index-errors i.e. rather than the whole show stopping just because an index is outside the length of a vector object the show just goes on without alerting the coder.
My response to a critic from this perspective is that you must handle your errors to know when they occur!
In other words: own your errors, don't blame the computer.
In other other words: you can't reference things outside a noun using clever index arithmetic, because the index arithmetic is wiser than you (don't go against math, work with math).
Also, a vector is not a list.
Which brings me to , as injection or affix.
injection is different in behavior for affix-ion
an injection usually makes things flat in the relevant way, an affix-ion can be seen as either a nesting (tuples of tuples to python people), or perhaps a type of injection at different locations.
The use of injection as a descriptor of the behavior of the dyad , is difficult to argue from a design perspective (it is not a friendly word to English speakers), but affix is also connotative of a potentially misleading perspective (that of nesting where no nesting has occurred).
Though the act of injection might not be easily suggested as it requires a left argument having a sort of permutation like structure.
For example,  (x;y),z might be interpreted as a command to put z at x in y so as to make a new flat vector i.e. to ammend y so as to make room for z.
There is also the potential of using integer notation to give a three command form to , so that x 0n2, y gives , an adverbial behavior.
It's not clear if this makes conceptual sense.
These subtle idioms of the language are to be developed as needed, and so far affix will probably do fine for the dyad , .

20150924T1602 Hardy, Littlewood, and Pólya Inequalities
I read this book a while back, and happened to be tying my shoes in my closet today when my gaze came upon it.
I realized immediately that all the work I've been doing to develop N could be put to good use, or to good test, by translating and interpreting the arguments and result in N and seeing where they fit inside the arguments given by Goodstein in RNT.
Most importantly, HLP consider their work to cover the elementary inequalities in use throughout real, complex, and functional analysis.
My perspective on real, complex, and functional analysis is significantly different from theirs and most modern mathematicians, so it should be interesting to see where I can fit their results in my head.
The good thing is that HLP are smarter and wiser than I am likely to ever be, and their results are given in their most fundamental form which is what is of direct interest to me and my notational language N.


20150924T1553 Quotes and Goodstein's Introduction of Zero in FCOM
The main thing to remember is to keep the main thing the main thing.
Time stamp everything, you never know when you need to know when.
Well done is better than well said.
If it's meant to be then it's up to me.

Goodstein waits until page 43 to introduce the number zero!
It's brilliant and genius.
He gives zero meaning as a number by introducing it with modular arithmetic, or what he more aptly calls 'arithmetic of remainders'.
This is a design feature of Goodstein's perspective and methods.
He doesn't just want to do as much as he can with as little as he can.
He wants to do it in a clean, clear, and vivid way.
A way that humans can follow and that seems to almost be self justifying at each step.
Goodstein's Fundamental Concepts of Mathematics is a must read for any math teacher at the middle or high school level.
Not because middle or high school teachers will cover the material in it (they should, but it's unlikely they would), but because of the vivid perspective it gives on the part math plays in everyday life.
It's the only book of its kind that I've ever read (and I've read A LOT of math books, like a whole lot, and when I say I've read them, I mean I really went through and read every page, something that a lot of people tend not to do, especially when the book is really popular).
It's also interesting to note that Goodstein uses the sign 0 for counting with an abacus using standard decimal notation, but there it is a place holder, and the digit by itself is not yet recognized as a numeral i.e. not clearly denoting a thing that we will perform arithmetical acts with.
The reason it makes sense to introduce zero with remainders is because you can think of remainder as what is left over after you've taken away as many collections containing the dividend number of objects from the collection being divided.
It's genius, because it means that zero can be interpreted as what's left over after everything has been taken away.
At the same time, by deferring its introduction we avoid a massive number of questions about how best to define, via recursion, the basic operations of arithmetic: addition, subtraction, multiplication, exponentiation, titration etc.
If you want a crystal clear description of the fundamental concepts of modern mathematics look no further than Goodstein's "Fundamental Concepts of Mathematics".



I want to build a list comprehension using the following code:
[[x,y] if 3!=x+y else [] for x in range(3) for y in range(4)]
but instead of inserting an empty list where the condition is not met, I want it to not do anything and just go on.

archive a message from Mail.app with control+command+A

Errors should be tracked across all areas of life.

Learn from your mistakes.
Don't find fault, find a remedy.

Download latest Python (3.5) pre packaged for mac
Invoke from Terminal with python3.5
The version of python invoked with 'python' is 2.7.10

Learning is an endless bootstrapping process.

http://brew.sh/
ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew update
brew install gcc
man gcc-5

tree/node notation
a
|\
b c
|\
d c
|
e

a.c
a.b
a.b.c
a.b.d
a.b.d.e
note, a.c ~= a.b.c (unless they match)
trees locally, graphs globally


colophon
John Meuser
Inconsolata
Solarized
Git
GitHub
HTML
CSS
ed
sed
TextEdit

projects
goals
purpose
jsource
music

dictionaries
key-values
hash
Algorithms
Data Structures
Programming Languages
Artificial Intelligence
Databases
Security
Distributed Systems
Operating Systems
Networking
Topologies
Protocols
Applications
Network Congestion
Network Resilience
Popular Operating Systems
Mobile Operating Systems
Special Operating Systems
Components
Client-Server
Map-Reduce
ACID
CAP
Concurrency
Synchronization
Cryptography
Hashing
Information Security
Network Security
Secure Coding
Authentication
Relational Databases
NoSQL Databases
Object-oriented Databases
Database Design and Modeling
Transactions and concurrency
Administration
Storage
Database Security
Machine Learning
Natural Language Processing
Deep Learning
Search &amp; Optimization
Reasoning
Classification
Statistical Learning
Game Theory
Popular Languages
Scripting Languages
Web Languages
Mobile Languages
Functional Languages
Esoteric Languages
Other Languages
Lists
Arrays
Trees
Hashes
Graphs
Sorting
Search
Recursion
Dynamic Programming
Greedy Algorithms
Strings
Graph Theory
Combinatorics
Number Theory
Bit Manipulation
Summations and Algebra
Probability
Geometry
Randomized Algorithms
NP Complete problems
Analytic number theory
Algebraic number theory
Probabilistic number theory
Enumerative combinatorics
Analytic combinatorics
Matroid theory
Probabilistic combinatorics
Algebraic combinatorics
Geometric combinatorics
Topological combinatorics
Arithmetic combinatorics
Combinatorial optimization
Discrete and computational geometry
Elementary Graph Algorithms
Minimum Spanning Trees
Single-Source Shortest Paths
All-Pairs Shortest Paths
Maximum Flow
Floyd-Warshall
Concatenation &amp; Substrings
Prefixes &amp; Suffixes
Rotations
Reversal
Ordering
Encoding
Representation
Parsing
Mining
Sequencing
Partitioning
Searching
Manipulation
Matching
Regular Expressionss
Pure Greedy
Orthogonal
Relaxed
Dijkstra's shortest path algorithm
Fibonacci sequence
Matrix Chain Multiplication
Longest Common Subsequence
Sequence alignment
Top-Down
Bottom-Up
Backtracking
Binary Search
Breadth First Search
Depth First Search
Combinatorial Search
Simple Sorts
Efficient Sorts
Bubble Sorts
Distribution Sorts
Basic Graph
Adjacency list
Adjacency matrix
Binary decision diagram
Directed graph
Directed acyclic graph
Multigraph
Hypergraph
Hash table
Hash list
Hash tree
Hash trie
Bloom filter
Distributed hash table
Double Hashing
Dynamic perfect hash table
Prefix hash tree
Space Partitioning Trees
Application Specific Trees
Binary Trees
B-Trees
Multiway Trees
Heaps
Tries
Bit array
Bit field
Bitmap
Dynamic array
Hashed array tree
Lookup table
Matrix
Parallel array
Sorted array
Sparse matrix
Variable length array
Linked list
Doubly linked list
Array list
Self-organizing list
Skip list
Doubly connected edge list
Difference list
Free list
VB .NET
Pascal
R
D
Groovy
Brainfuck
LOLCODE
WhiteSpace
Scala
Haskell
Clojure
Erlang
F#
OCaml
Racket
Common LISP
SWIFT
Objective C
Php
Javascript
HTML5
Perl
Lua
C
C++
Java
Python
Ruby
C#
Question answering
Sentiment Analysis
Speech Recognition
Text-to-Speech Conversion
Named entity recognition
Decision tree learning
Association rule learning
Artificial neural networks
Inductive logic programming
Support vector machines
Clustering
Bayesian networks
Reinforcement learning
Representation learning
Similarity and metric learning
Sparse dictionary learning
Genetic algorithms
Cassandra
HBase
MongoDB
DB2
PostgreSQL
Microsoft SQL Server
MySQL
Symmetric-key cryptography
Public-key cryptography
Cryptanalysis
Cryptographic primitives
Cryptosystems
Kernel
File Systems
Memory Management
Process Management
Distributed Operating Systems
Network Operating Systems
Object Oriented Operating Systems
Embedded Operating Systems
Android
iOS
Windows Phone
Linux
OSX
BSD
UNIX
Windows
IPTV
Videoconferencing
Online games
VoIP
Routing Protocols
Secure Protocols
TCP
IP
HTTP
POP
IMAP
FTP
Basic Trie
Radix tree
Suffix tree
Suffix array
B-trie
Binary heap
Weak heap
Binomial heap
Fibonacci heap
Ternary tree
Disjoint-set
Fusion tree
Fenwick tree
Basic B-tree
B+ tree
B* tree
2-3 tree
2-3-4 tree
Queap
Binary Search Tree
Self balancing Tree
Red-Black Trees
AVL tree
Splay tree
Weight-balanced tree
Abstract syntax tree
Parse tree
Decision tree
Minimax tree
Segment tree
R-tree
Counting sort
Bucket sort
Radix sort
Bubble sort
Shell sort
Comb sort
Mergesort
Heapsort
Quicksort
Insertion sort
Selection sort
Naive String Matching
Rabin-Karp
Finite Automata
Naive string search
Finite State Automaton
Index
Fuzzy Searches
Breadth First Search
Depth First Search
Dijkstra's Shortest Path
</pre></body></html>
