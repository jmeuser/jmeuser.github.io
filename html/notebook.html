<html><head>
 <link rel="stylesheet" type="text/css" 
href="http://fonts.googleapis.com/css?family=Inconsolata">
 <link rel="stylesheet" href="../css/style.css" type="text/css" media="screen">
</head>
<body><pre>
John Meuser's Notebook
These are clumps of raw unprocessed thought dredged into words and maybe even 
sentences.
They are organized as a stack i.e. the most recent note is at the top.

20151003T2055 Rehashing Notes on ^ and Factorial Powers Example

I’ve been spending a lot of time recently working on N notation for primitive 
recursion beginning with the following “natural” definition:

(f^0 n) ~ n
(f^(S m) n) ~ f^m f n

This conforms to classical iterative notation for repeated application of a 
function in a fixed point manner.

The extension of this operation is where I believe primitive recursive 
definitions can be introduced simply and with a sort of fitting surprise, I 
just have yet to find the best or most surprising form.

In N, J, and k the following notation is frequently used without concern for 
the space needed to perform the computation:

+/ f !4
+/ f 0 1 2 3
+/ (f 0), (f 1), (f 2), f 3
(f 0) + (f 1) + (f 2) + (f 3)

This is the notation that will eventually replace classical summation notation 
(and product notation etc.).
But, as you can see it first stores all the values of f at each numeral 0 1 2 3 
before summing it.
Hence the actual utility of the expression +/f (which you could read as “sum 
over f of”) is best used on pre-existing data.

I think that ^ can be used, in a natural way, to provide the same facilities 
without taking up any space other than what is needed to compute each step of 
the sum:

(+^f 1) ~ f 0
(+^f S n) ~ (+^f n) + f n

So that

(+^f 4)
(+^f 3) + f 3
(+^f 2) + (f 2) + f 3   (this being when you could compute the sum of f.2 and 
f.3 and simply store the result and continue the iterate calculation)
(+^f 1) + (f 1) + (f 2) + (f 3)
(f 0) + (f 1) + (f 2) + (f 3)

Thus the factorial powers “x to the k falling” and “x to the k rising” (which 
occur frequently in CS and numerical math) would be written as:

x *^- k
x *^+ k

respectively if the following conventions are followed for a pair of dyadic 
verbs (binary functions) f and g:

(m f^g 1) ~ m g 0
(m f^g S n) ~ (m f^g n) f m g n

For example

x*^-3
(x*^-2)* x - 2
(x*^-1)* (x - 1) * x - 2
x * (x - 1) * (x - 2)

There are still some “notational kinks to work out” but this method of 
combining binary operations via the classical “power of” operation has been a 
long time coming.

One reason for abandoning the use of ^ as “exponent” is because the existence 
of roots of real numbers is, I believe, a highly suspicious belief to hold with 
any certainty for, as far as I know, it is still unknown whether or not there 
is a primitive recursive real number which cannot have a primitive recursive 
expansion. In other words, when it comes to sitting down and calculating roots 
via rational root approximations we have not yet established with certainty 
whether there might be a primitive recursive real number (i.e. a “real number” 
whose rational approximations are calculable with pen and paper) which does not 
have a primitive recursive expansion (i.e. pick a scale of measurement, like we 
do with decimals, and try to write out successive decimal approximations to 
that real number). This startling fact is one reason to consider using ^ for 
composing arithmetic operations rather than as an arithmetic operation in 
itself.

To contradict my last statement I do allow for numeral arguments to both sides 
of ^ and expect they should abbreviate the following:

 2^3
2 2 2

 2^(4 4)
2 2 2 2
2 2 2 2
2 2 2 2
2 2 2 2

 5^(3 3 3)
5 5 5
5 5 5
5 5 5

5 5 5
5 5 5
5 5 5

5 5 5
5 5 5
5 5 5

which is a 3 by 3 by 3 “brick” (as it is referred to in J) all of whose atoms 
are 5.
Functions to rectangular arrays behave as follows:

 f^(3 3) n
(f^3 n) , f^3 n

 f^(3 3; 3 3) n
(f^3 n), (f^3 n);
(f^3 n), (f^3 n)

The reason for these conventions comes from the analysis of a multidimensional 
array via its frames, cells, items, and atoms.

A practical/theoretical reason for me to obsess over these silly notational 
things is to be found in Hardy, Littlewood, and Polya’s Inequalities. If you 
flip towards later chapters, or even earlier chapters, you find that they are 
hardly doing tensor analysis and yet primitive inequalities are thwarted by 
their own notation.



20151002T1416 Less Than, Greater Than and N

(remember - means monus in N not minus e.g. 0 ~ 1-5)

In the current version of N notation the signs &gt; and &lt; are the classical 
operations of max and min respectively e.g.

 3&gt;5
5
 3&lt;5
3
 300&gt;432
432

This is in stark contrast to the classical interpretation of &gt; as greater 
than and &lt; as less than.
In N, for any numerals x and y we have

(x &gt; y) ~ x + y - x
(x &gt; y) ~ y + x - x
(x &lt; y) ~ x - x - y
(x &lt; y) ~ y - y - x

From which it follows that

(x &gt; y) ~ y &gt; x
(x &lt; y) ~ y &lt; x

These being STATEMENTS ABOUT numerals rather than an abbreviation for A numeral.
One could use these statements to say "max and min are commutative" but one 
would NOT say "max and min are reflexive" for reflexivity is a property of 
relations and, in N, &gt; and &lt; are not relations.
(now, yes, you can construct a relation or interpret them as relations, but if 
you fit these operations into Goodstein's Equation Calculus then you will see 
that they are clearly just abbreviations for numerals and not "relations")

Given a pair of numerals x and y to say "x is greater than or equal to y" we 
could write:

x ~ x + y - x
x ~ x &gt; y      N.B. "x is the same as the max of x and y"

It is not yet clear to me if allowing ~ to be defined in different ways by the 
user is a good design idea or a bad one, but I've been toying with the idea of 
giving people the power to redefine ~ as they see fit so that they can project 
statements about arithmetic onto arithmetic as they see fit.

In N's current system it is easiest to say that a statement (x ~ y) is "True" 
when the sgn of their positive difference is 0 and "False" when the sgn of 
their positive difference is  1.
An alternate way of looking at this, using just positive difference, is to say 
that a statement (x ~ y) is true if the positive difference of x and y is 0 
otherwise, we are given shades of Falseness depending on how big the positive 
difference is or how far it is from 0.
So for now we might think of ~ as an abbreviation for *= which maps statements 
about arithmetic into arithmetic (that ~ is the only symbol required to build a 
complete formalization of all elementary number theory via primitive recursive 
functions is the whole point of Goodstein's equation calculus where he uses = 
for ~ and |x,y| for the positive difference, I've just done the proper thing 
and simplified his concepts using hindsight).
 
~ : *=  (read as "same is signum positive difference" or "same means the claw 
of signum with positive difference" or "same abbreviates signum positive 
difference" or some as yet documented interpretation)

Thus ~ is an abbreviation for the claw *= which is sgn pd so that
 0 *= 0
0
 0 ~ 0
0
 12 *= 6
1 
 12 ~ 0
1

Under this interpretation of ~ we can define the relations leq (less than or 
equal to) and  geq (greater than or equal to) to give numerals as well

leq:{x ~ x - x - y}
leq:{x ~ x &lt; y}
leq:{x}~&lt;           N.B. This last definition is a fork of the left 
identity, same, and max

 3 leq 4
0

(
In J the left identity is given by [ and the right identity is given by ] these 
are the simplest of the immensely useful identity/projection functions 
introduced and used with great effect by Gödel and Kleene in their work on 
mu-recursive functions.
I'm not sure if I should use them in N for the same reason, or if they should 
be reserved for the now familiar index notation so often found in modern 
programming languages.
In this case the definition leq:[~&lt; would be shorter and read "leq 
abbreviates left identity same min" which could be expanded from Pidgin English 
to "leq is an abbreviation for the left identity being the same as the minimum".
One can actually define left and right identity from plus and monus so that

LI:{(x + y) - y}
RI:{(x + y) - x}

This suggests that one should consider the primitive patters of all expressions 
having the form

u f v g w
(u f v) g w

where u,v,w are one of the numerals x or y and f,g are one of the verbs + or - 
(plus or monus).
)




20151002T1227 Why use ^ rather than / ? Or what relationships are there between 
^ and / with !  ?

If space is not a concern, and if you are inclined to view things in a 
classical way then the following way of calculating is more appropriate:

+/!4
+/ 0 1 2 3
0 + 1 + 2 + 3
0 + 1 + 5
0 + 6
6

Where as, with at least one of the definitions for ^ redo given below, you 
could write this as:

0 +^+ 4
(0 +^+ 3) + 0 + 3
(0 +^+ 3) + 3
(0 +^+ 2) + 0 + 2 + 3
(0 +^+ 2) + 0 + 5
(0 +^+ 2) + 5
(0 +^+ 1) + 0 + 1 +5
(0 +^+ 1) + 0 + 6
(0 +^+ 1) + 6
(0 +^+ 0) + 0 + 0 + 6
(0 +^+ 0) + 0 + 6
(0 +^+ 0) + 6
0 + 0 + 6
0 + 6
6

The previous sequence of events constitutes a calculation using one of the 
definitions for ^ given below.
It is not the most efficient definition, and clearly this execution shows how 
one should and could improve the definition.
It is a good example of how one might hope to relate ^ and / as follows

(+/f!n) ~ +^f n

In other words, if you you wanted to add the values of f from 0 to n-1 then you 
could do it in one of two ways:

The +/f!n way:
[1] list the arguments you want to give to f using (!n)~ 0,1,2,3,4,..,n-1
[2] Calculate f of !n i.e.
f!n
f 0,1,2,..,n-1
(f 0),(f 1),(f 2)..,f n-1
[3] sum over these values:
+/f!n
+/f 0,1,2,..,n-1
+/(f 0),(f 1),(f 2),..,f n-1
(f 0) + (f 1) + (f 2) + .. + f n-1

The +^f n way:
[1] Apply definition of verb +^f to n
+^f n
(+^f n-1) + f n-1
(+^f n-2) + (f n-2) + f n-1
..
(+^f 0) + (f 1) + (f 2) + .. + (f n-2) + f n-1
(f 0) + (f 1) + (f 2) + .. + (f n-2) + f n-1

Notice that if you were actually calculating with +^f instead of +/f you can 
compute each value of f as it is needed and not before.
When using +/f!n you must generate enough space to keep all the values, so that 
at any step in the calculation you generate all the data you will add all the 
way up until the end.
I think the similarities are undeniably important.
Specifically, the case when you would use the form +/f is if you have some data 
in an array A and you want to sum its transformation through f i.e. +/f A.
If you do not already have an array of data A and you have to generate it for 
the purpose of making a current computation then you can do it in place using 
+^f n although it is likely that you will have to make a transformation from n 
via a helper function g before f so that the final form might look more like 
+^f.g n so that at each step you get 

+^f.g n
(+^f.g n-1) + f.g n-1
(+^f.g n-2) + (f.g n-2) + f.g n-1
and so on.

It doesn't make much of a difference on small data sets (as is often the case 
with most operations), but the difference on huge data sets is profoundly 
different.
For one the form +^f needs only as much space as is needed to accumulate the 
sum + at each step, where as +/f goes over a potentially huge set of data in 
order to summarize it by a single numeral.

It's important to recall that the "classical" motivation for using ^ in this 
way is from iteration of a monadic (unary) verb (function):

%^2 300     N.B. % means "integer square root" and %^2 "integer square root 
repeat two"
%^1 % 300
%^1 17
%^0 % 17
%^0 4
{y} 4   N.B. {y} means "the right argument"; projection of right argument; 
identity
4

Many people are most familiar with using ^ to indicate the exponential 
function, but in N if you were to use ^ with two numeral arguments you might be 
surprised (or not):

 2^3
2 2 2
 5^6
5 5 5 5 5
 5^(3 3)
5 5 5
5 5 5
5 5 5

It just repeats (or copies) the left argument by the right argument.
This gives an analogy making sense with functions:

 f^3 x
f f f x
 f^5 x
f f f f f x
 f^(3 3)
(f^3 x) , f^3 x
 f^(3 3; 3 3) x
(f^3 x),(f^3 x);
(f^3 x),(f^3 x)

Though the last few showcase that behavior on functions is a bit different when 
the right argument is a rectangular array of numerals.
Interestingly, or perhaps not, the following calculate the same value using 
these notational conventions:

 f^(3 3; 3 3) x
(f^3 x),(f^3 x);
(f^3 x),(f^3 x)

 f^3 (x,x);x,x

The adverb rank " should be used in order to deal with the application of f^3 
on different frames of its argument.
There is still a lot of work for me to do on organized array actions.
There are a lot of conventions, and so far I am most pleased by Iverson's use 
of cells and frames in J via the rank " adverb (operator).
The only way to really get frames, cells, atoms, and items of rectangular 
arrays (and eventually trees) to work "right" is to see how they're used in 
Applied Analysis.
Specifically, the use of rectilinear arrays as tensors in simulations and 
approximations of physical phenomenon.

As I said earlier, the use of ^ is most often associated with exponentiation.
In general, exponentiation is actually a very different operation from its more 
familiar "power of" denotation.
Specifically, people think of 2^4 as "two to the fourth power" or "two to the 
power of four" and from this definition "to the power of" is repeated 
multiplication of the left argument by the right argument number of times.
So in classical notation one would calculate 2^4 as follows:

2^4
2 * 2 * 2 * 2
2 * 2 * 4
2 * 8
16

With N's current notation for ^ , power is calculated as follows:

(2*)^4 1
2* 2* 2* 2* 1
2* 2* 2* 2
2* 2* 4
2* 8
16

So that the thing which is repeated four times is the act of multiplying by 2.
The verb (function) "multiply by two" is written in N as 2* and for now the 
parenthesis around it in (2*)^4 1 are needed so that ^ can distinguish it from 
2 *^4 1 which currently has no clear meaning in N's notation.

People might find it clumsy to use (2*)^4 1 instead of the classical 2^4 which 
seems much simpler.
It is easy to to define power using E as a binary verb:

 E:{(x*)^y 1}
 2 E 4
16

Why not just use the classical notation?
The reason is not only do you gain the ability to use ^ in all those settings 
where you are "repeating" any sorts of operations on any sorts of arguments 
(something which cuts to the heart of arithmetic and math in general), but, in 
general, exponentiation is hard outside of elementary number theory.

If you open a book on real analysis you'll find the first chapter (or maybe 
more) dedicated to a characterization of the set of real numbers.
For example, in Rudin's well known Principles of Mathematical Analysis, the 
real numbers are introduced as an ordered field having the least upper bound 
property and containing the rational numbers.
Furthermore we are told (it is proven) that any other ordered field with the 
least upper bound property and containing the rational numbers is isomorphic 
(read "the same as") with the real numbers.
The real numbers are "constructed" by Rudin using Dedekind Cuts (in actuality 
he uses Russell's refinement of Dedekind's original construction: Russell 
noticed that you don't need to keep the left and right side of a cut, you only 
need one side and that's enough for the same exact argument to go through).
The construction proceeds from the set of rational numbers by first collecting 
all those subsets of the rationals that are Dedekind Cuts (they have certain 
properties that are obviously similar to those you are looking for in an 
ordered field with the least upper bound property).
He then proves that the arithmetic of rational numbers can be extended to these 
cuts and completed to satisfy the requirements of an ordered field having the 
least upper bound property.

From the characterizing properties of the real numbers as an ordered field with 
the least upper bound property that contains the rational numbers it is proven 
that it is possible to defining an operation of "n-th root" of every real 
number.
The proof of existence of a number satisfying the property of an n-th root is 
given by first constructing a bounded set and showing that its least upper 
bound is the unique number satisfying the property of an n-th root.

My contention is that this construction is highly misleading, and hardly 
"gives" the real number that is the n-th root.
In fact, whenever we take the n-th root of a number we're always doing it with 
a rational number because we can not write out by hand anything but a reference 
to most "real numbers".
Said another way, we can claim that x is a real number, which means it might be 
defined via any number of logical statements, but we may have little to no 
knowledge of how to approximate or "realize" x as some kind of abbreviated 
quantity.

Alternatively, one can proceed down Goodstein's Recursive Analysis.
There is also Bishop's Constructive Analysis, but I have yet to read it again 
after having entered on such a detailed study of Goodstein's works.

Back to the exponentiation: in N and in Goodstein's system, if you want to give 
the n-th root of a number (specifically of a rational number), then it must be 
given via a primitive recursive function, something which is easily calculated 
and easily computed.
The need for ease of calculation aids in any proof that the procedure gives the 
desired quantity, but also lends to insight into managing the calculation of 
sometimes hard to get numerals.

The use of sets to "build" a number is idealistically valid, but the practical 
construction of numerals is less a matter of collections and more a matter of 
proper abbreviations and algorithms for their efficient manipulation.
This is true EVEN in the case of using sets and not just numerals and function 
signs for operations on numerals.

In the future, it will hopefully be easier to see that one can recreate the 
real numbers and their various operations, by crossing the proper ordinal 
barrier via an argument by transfinite induction.
This is likely to have the benefit of making certain analytic arguments (in the 
classical sense) more the product of primitive recursive analysis.

It's at this point that I must remark: this document is called "collect" for a 
reason.
It is nothing more than a collection of raw and unprocessed thoughts.
It is a way of thinking out loud in an attempt to discover what it is that I 
have to say about different topics with the intent of giving focus to my future 
thoughts on the investigated topics.



20151001T1725 Conditional Iteration

(f$g y) gives y if (0 ~ g y) else f$g f y

If I adopt this notation then it would be read "f if g of y"



20151001T1619 Collecting Definitions for ^ redo possible expansion to $ because 
recursion is really important

Over the past few days, and perhaps throughout the past year, I've come up with 
a number of different way to think about using notation for recursion, some 
much better than others.
Right now I'm just focusing on my work with N and what I'm currently calling 
the Redo operation ^.
Here are the definitions collected into a single place.
The definitions here assume that f and g denote dyadic verbs (binary operations 
or binary functions (but not really in N since everything is actually an 
abbreviation for a numeral or an abbreviation for a verb):

Definition A
(0 f^g y) ~ y
(x f^g y) ~ (x g y) f^g x f y

Definition B
  (0 f^g y) ~ 0 g y
(S.x f^g y) ~ (x f^g y) g x f y

Definition C
  (x f^g 0) ~ x g 0
(x f^g S y) ~ (x f^g y) g x f y

Definition D
(x f^g y) gives y if (0 ~ x g y) else x f^g f y
(f^g y) gives y if (0 ~ g y) else f^g f y

Definition E
(x f^g y) gives y if (0 ~ x) else (x g y) f^g f y



20151001T0811 Finally Folding Long Lines
Up until today the lines of this website ran all the way to their end.
Now they don't because I added fold -s to the shell command I use to transform 
my text files into html files.
It doesn't produce the prettiest output yet, but it's a step in the right 
direction.
As I've said before I can not justify putting time into making things pretty 
when there aren't enough things to know what pretty might mean with regards to 
them in the first place.

20150930T1422 another possible definition of redo ^
The definition for ^ given yesterday were as follows:

S successor

f:unary
n:numeral
y:numeral
f^0 y gives y
f^(S n) y gives f^n f y

f:binary
g:binary
x:numeral
y:numeral
0 f^g y gives y
x f^g y gives (x g y) f^g (x f y)

Today I present an alternate form of evaluation in an attempt to make the 
closets connection to the classical form of recursion that most are familiar 
with.
Anytime I say recursion I mean primitive recursion unless otherwise noted for 
general recursion allows methods of calculation which rely on logical proofs in 
certain logical systems where as my goal is to follow Goodstein by sticking to 
arithmetic first and deriving logical operations as a consequence of arithmetic.
Due to the nature of recursion as revealed by Peter in her Recursive Functions, 
many forms of recursive definition are reducible to primitive recursion in one 
argument without parameters.
For example, course of values recursion is eliminable to primitive recursion in 
a single variable without parameters via the fundamental theorem of arithmetic.
Finding the correct form of definition for f^g means giving one which hints, as 
much as possible, to practical forms of these powerful reductions.

The form of definition today is derived more directly from the definition by 
recursion as given by Goodstein on pg.19 of RNT:

"
 F(x,0)=a(x)
F(x,Sy)=b(x,y,F(x,y))
" Goodstein RNT pg. 19

I've given his scheme for definition by recursion in his notation.
In N one would write these expressions as:

  (0 F y) ~ a y
(S.x F y) ~ (x F y) b x,y

or

    0.F.y ~ a.y
(S.x).F.y ~ x.F.y b x,y

I prefer the former to the latter for now.
First, the reason the recursion in N is written so that it is a recursion in x 
rather than in Goodstein's definition where it is recursion in y.
In N, the convention is that x is used most often to refer to the left argument 
to a dyadic verb, where as y usually refers to the right argument of a dyadic 
or monadic verb.
A further convention which has far reaching implications for the way in which 
we think about recursively defined verbs is the use of the left argument as the 
argument in which the recursion is in.
The reason is because in order to better express the functional relations 
needed to successfully retain analogs to classical analytic results when using 
only primitive recursive rational functions it becomes paramount to imagine a 
family of functions index by a natural numeral.
In N an expression such as 3.f or (3 f) gives a projection of a dyadic (binary) 
function (verb) f whose left argument is 3.
For example

 f:+     f is plus
 3.f     3 of f
3+       3 plus
 g:3.f   g is 3.f
 g 4
7
 3+ 4
7

Many functions in primitive recursive analysis are used via similar forms in 
majorizing arguments which play the role of classical limit arguments.
The relatively exponential function E is defined recursively from the power 
function (pow) and the factorial function (fac) so that

n numeral
y rational

  (0 E y) ~ 1
(S.n E y) ~ (n E y) + (y pow S.n) % fac S.n

So that for a given problem, one may need only 3.E or perhaps 5.E or n.E for 
some n to carry through an argument or computation.
The advantage being not only all the machinery of primitive recursive analysis, 
but immediate computability for experiment or application.

Upon reviewing the definition I gave yesterday I realized its symmetry was not 
properly reflective of the form of recursive definition more commonly used.

An alternate way of interpreting f^g for a pair of dyadic (binary) verbs f and 
g is as follows

  (0 f^g y) ~ y
(S.x f^g y) ~ (x f^g y) f x g y

As is easily seen, this more closely reflects the statement of Goodstein's 
schema for definition by primitive recursion given above (and reproduced her 
for immediate comparison:

      0.F.y ~ a.y
  (S.x).F.y ~ x.F.y b x,y

  (0 f^g y) ~ y
(S.x f^g y) ~ (x f^g y) f x g y

Or, perhaps to better complete the established pattern to the base case:

  (0 f^g y) ~ 0 g y
(S.x f^g y) ~ (x f^g y) g x f y

This suggests that the unary form of f^g be defined via the following 
substitution into its binary counterpart:

(f^g y) ~ y f^g y

So that the factorial function is defined as:

fac: *^{1+x}
fac 3
*^{1+x} 3
3 *^{1+x} 3
(2 *^{1+x} 3) * 2{1+x}3
(2 *^{1+x} 3) * 1+2
(2 *^{1+x} 3) * 3
(1 *^{1+x} 3) * (1{1+x}3) * 3
(1 *^{1+x} 3) * (1+1) * 3
(1 *^{1+x} 3) * 2 * 3
(0 *^{1+x} 3) * (0{1+x}3) * 2 * 3
(0 *^{1+x} 3) * (1+0) * 2 * 3
(0 *^{1+x} 3) * 1 * 2 * 3
(0{1+x}3) * 1 * 2 * 3
(1+0) * 1 * 2 * 3
1 * 1 * 2 * 3
1 * 1 * 6
1 * 6
6



20150929T1522 redo ^
First, ^ is referred to as “redo” or “repeat” it is the entry point for 
“recursion” into N notation.
For a numeral n and unary function f

f^0 y gives y
f^(S n) y gives  f^n f y

In other words, it’s just iteration (where S is the successor operation).
Now here’s where things get fun, and perhaps profoundly interesting.
The generalization of the operator ^ for two binary functions f and g is as 
follows

0 f^g y gives y
x f^g y gives (x g y) f^g (x f y)

Thus if P is the predecessor operation then

(S n) f.{y}^P.{x} y gives n f.{y}^P.{x} f y (after computation where {y} is 
projection of the right argument and {x} is projection of the left argument).

Which is the same as simple iteration in the form (f^n y)

If f and g are both unary we can make the agreement that

0 f^g y gives y
x f^g y gives g.x f^g f.y

Which lets us write

n S^P.P 0

for the floor of half of n (or the integer half of n).

A function whose recursive definition is classically given via a helper 
function:

alt 0 gives 0
alt S n gives 1 - alt n  (here - is monus i.e. m - 0 gives m and m - S n gives 
P m-n)

so that alt 0 gives 0, alt 1 gives 1, alt 2 gives 0 and so on.

then

hf 0 gives 0
hf S n gives (hf n) + alt n

where hf is the the integer half function.

Here is probably a more “likable” definition

n *^P 1

is the factorial of n (where P is the predecessor and  * is normal 
multiplication).
The following sequence of events showcases how you might imagine working with 
this notation:

4 *^P 1
P.4 *^P 4*1
3 *^P 4
P.3 *^P 3*4
2 *^P 3*4
P.2 *^P 2*3*4
1 *^P 2*3*4
P.1 *^P 1*2*3*4
0 *^P 1*2*3*4
1*2*3*4
24

Similarly

n +^P 0

is the sum of the numbers from 1 to n.
The importance is that these are recursive definitions.
One can use a different notation, the unary numer function !, to achieve the 
same things

!2 gives (0,1)
!3 gives (0,1,2)
!10 gives (0,1,2,3,4,5,6,7,8,9) 
and so on

so that the factorial of n can be given as

*/1+!n

which is times over numer n.

*/1+!3
*/1+(0,1,2)
*/(1,2,3)
1*2*3
6

Which is a more “explicit” method of calculating with factorials (this list the 
way factorial is usually given in text books when people write (fact n) = 1 * 2 
* 3 * 4 * … *n

(x*y) is the same as x+^y 0
(x exp y) is the same as x*^y 1


20150929T1251 Refining the primitive concepts of N
The greatest contribution to my development of N has been identifying the 
dyadic adverb ^ as redo|again|repeat.
I've settled, for now, on redo because it is appropriately vague on how you 
will redo the next step, perhaps with some minor edits or changes (which is 
what happens in most places where it might be used).
Though, in a barely second place to ^ as redo, are the following 
definitions/identifications:

&gt; max
= pd
&lt; min
&amp; gcd|meet
| lcm|join
~ same

Though, as will hopefully become clearer as I develop N and my presentation of 
its utility, the identification of append|affix|concat which is denoted , as a 
binary operation (dyad) should have the greatest impact on how mathematicians 
view classical expressions such as f(x,y,z) (mostly because this notation 
maintains its classical meaning in N's new interpretation).

I'll give a brief explanation of what = means (it's not that complicated 
really).

 2=1
1
 5=9
4
 9=5
4
 105=99
6

The dyadic verb = is called "positive difference" and is the most natural 
"norm" on the natural numbers.
My contention is that the sign = should no longer be used to denoted equality 
because that concept is poorly defined in most cases.
Specifically, = should be included among the arithmetical signs and not among 
logical signs.
That is where ~ comes in, as same, it is much more appropriate, in general, for 
expressing the similarities that we are actually used to seeing in algebraic 
expressions.

Furthermore, there is a general outlook on arithmetic which is very clearly and 
exactly described in Goodstein's Recursive Number Theory that supports the use 
of = as positive difference.
Specifically, it is important to prove or "know" that if the positive 
difference between one numeral and another is zero then they must be the same 
numeral.
This has far reaching implications in how we conceive of sameness inside a 
theory and outside a theory.
In other words, it is important to know where arithmetic ends and where our 
statements about arithmetic begin.
Though it might sound like a theoretical issue only, it is not.
Ask any computer scientist and you will see that the notion of equality is 
usually a matter of "relevant taste".
I've decided to subsume equality as a concept under the moniker "same" for now, 
though it might stick as I go along.

In the other notation of N the positive difference satisfies the following 
identity:

{x=y} ~ {(x-y)+y-x}

Though this is given in the curly bracket notation (something which I'm still 
trying to decide whether it helps thinking or if its just a redundant crutch).

To put into symbols the statement that two numerals whose positive difference 
is zero are the same we write:

(0 ~ x = y) ~ x ~ y

To some people, this might be profound (because it is) and to others it might 
seem silly because of how simple it is (because it is).

Why make &lt; and &gt; return min and max respectively?
There are at least two arguments: one is "practical" the other is 
theoretical|design based.
The practical argument is that deciding whether one numeral is greater than or 
less than another is a statement about numerals, where as giving the the 
maximum of a pair of numerals is an arithmetic action.
Another practical observation is that no matter where you might be 
"incrementing or decrementing" (something that you shouldn't be doing with N in 
the first place, but that's something completely different to write about) you 
can use the this claw *= to do what you've really been wanting.
For example

 1 *= 3
1
 2 *= 3
1
 3 *= 3
0
 5 *= 1
1
 4 *= 2
1
 3 *= 3
0

There is also the situation that frequently someone wanting to "compare" 
numbers are doing so because they desire to find the minimum or maximum of them.
It's different, but it's not different to be different.
It's different because it's something that cuts closer to the "heart" of 
arithmetic and its relation to statements about arithmetic.
This being the theory argument (the details of the theory are easily read in 
Goodstein RNT, but because people are afraid of thinking I've not gone into 
detail here so as not to scare them off).
Suppose you wish to decide whether one number is greater than another.
Supposing you believe 0 to represent "True" and 1 to represent "False" you 
could write

less:{x=x&lt;y}
 3 less 4
0
 4 less 3
1


Suppose you believe 0 to represent "False" and 1 to represent "True" you could 
write

less:{1-x=x&lt;y}
 3 less 4
1
 4 less 3
0

"Why do this!" you ask!!?
Because, orderings and decision is actually hard in general.
It is highly dependent on representation.
Under certain conditions you may find it necessary to redefine what it means 
for things to be less than or greater than: even when it comes to something so 
primitive as the natural numbers!
An example, but perhaps not the most accessible example, is the modular 
representation of integers described by Knuth in TAOCP Vol 2 Semi-Numerical 
Algorithms.
"But! Giving the max requires you to make a decision as to which is greater!"
No, it doesn't: consult the identity that follows.

(x &gt; y) ~ x+y-x
(x &gt; y) ~ y+x-y

Remember, in N, the dyad - is monus, not minus.
Minus is actually a powerful abstraction, where as monus is a familiar activity 
(because you can't take an apple from an empty bag and produce a negative 
apple).
The behavior of - on integer numerals is the operation of minus that everyone 
is happy being familiar with (for now).

The theoretical reason to considering = &lt; &gt; as pd, min, max is because 
these arithmetic operations are prior to the relations of equality, less than, 
and greater than.
It is a subtle but essential distinction, one which is bound to frustrate and 
baffle those who are not willing to entertain such a fundamental change in 
perspective.

There is an even deeper reason for defining things this way.
Propositions about arithmetic and arithmetic should be separated as much as 
possible so that it is easier to interpret propositions about arithmetic in 
arithmetic in ways that we have yet to imagine.
For example, there are different ways we can think of "ordering" the natural 
numbers.
Perhaps we want to imagine all the even numbers coming before the odd numbers!
Our imagination in this regard could change on a whim, but by agreeing on 
x&lt;y as an abbreviation for x+y-x or y+x-y we are focusing on how the 
fundamental arithmetic operations of plus and monus combine with each other.
In other words, whatever it is that we might wish from an arithmetic, we seem 
to be bound to introduce the concept of plus and monus from which the expression

(x+y-x) ~ y+x-y

is satisfied by any numerals x and y.
The fact that I have called &lt; min and &gt; max already "plays favorites" to 
the classical ordering of the natural numerals, but this is a design feature so 
that users are not immediately confused.

Another way of thinking about these thing is to see that the classical notion 
that a numeral x is less than or equal to a numeral y is encapsulated in the 
following proposition:

x ~ y-y-x

Again, this might seem silly, but, much like the makers of the C programming 
language said, it wears well as one uses it.
It opens doors from the beginning without overwhelming the user of mathematics 
with unnecessary or overwhelming choices.
One might think of it as a more "neutral" way of doing arithmetic.
A erudite would call this perspective "Post-Gödelian Arithmetic".
The reason?
All of these seemingly annoying redefining of age old concepts bare out all the 
way to the limits of arithmetic.
Specifically, to the point where arithmetic can be used to say that "There is 
an equation 0 ~ f n which is verifiable but not provable in our primitive 
arithmetic".
This seemingly abstract fact is actually considerably concrete, and the 
arithmetic conventions of N take this concrete fact into account from the 
beginning.
This fact follows form a specific interpretation of propositions about 
arithmetic as parts of arithmetic (Gödel Numerals).



20150927T1517 Aren't You Worried about someone stealing your ideas?
Yes and no.
I am worried about people not giving credit where credit is due e.g. 
Goodstein's work is largely overlooked even though he, like Russell, Hilbert, 
and others, has produced a monumentally important contribution to the 
foundation of mathematics and mathematical philosophy.
If someone copies my ideas then at least there will be detailed and public 
records that my work has been done by me over an iterated period of time.


20150927T1512 Primitive Inequalities
Any inequalities from Hardy, Littlewood, and Pólya Inequalities which are 
provable or derivable inside Goodstein's Primitive Recursive Arithmetic are to 
be called Primitive Inequalities.
Though there is the suggestion that they should be called elementary, there are 
reasons for not favoring this phrase which clash with the use of elementary to 
describe a class of functions.
It is important to identify which inequalities from Hardy, Littlewood, and 
Pólya Inequalities are provable in Goodstein's system as they are the principle 
relations used to establish what are commonly accepted as essential analytic 
results.
Comparing where these inequalities are used in Goodstein's Recursive Analysis 
will reveal the different branches of analysis that are united and divided by 
Goodstein's methods.


20150927T1451 The Design of Distributed and Parallel Computing Systems
We lack a practical foundation for parallel and distributed computing systems.
This is not because we lack the knowledge needed to understand these topics, 
rather we lack the proper narrative to carry this knowledge to those who need 
it.
Those who need it is a subjective group, but, it is my contention, that 
elementary school students need such knowledge.
They don't need knowledge of parallel computing, but they need the notation to 
describe their naturally developing tabular thought.
Humans think recursively and act iteratively.
Iterative actions can be parallel or "distributed" (not exclusively).
The elementary school use of phrases like "do the same thing again" or "repeat 
that but now on this one" is just one of the ways in which we introduce 
parallelism (or perhaps just concurrency) into the minds of elementary school 
students.
Multidimensional arithmetic is just one access point for putting parallel 
concepts into the hands of those who need it most.


20150927T1440 Goodstein and Iverson and Peter
I've written much on how the works of Goodstein and Iverson just work together.
There are some hints, though nothing explicit, that Iverson was aware of 
Goodstein's work but did not care to mention it.
Specifically, the writings in Goodstein's "Fundamental Concepts of Mathematics" 
are such that Iverson would have been not only attracted to its content but 
also its conceptual perspective.
One thing which is striking is that Goodstein's FCOM was published in 1962 and 
Iverson's "A Programming Language" was published in that same year.
In Goodstein's FCOM he uses the phrase "pronumerals" which is also used by 
Iverson in APL.
I could not find a single mention in Iverson or Goodstein of the others work.
I continue to find statements of having "making computation a mathematical 
activity" in papers and publications without seeing any mention of the works of 
Peter or Goodstein.
Goodstein makes it clear, multiple times throughout his work, that his efforts 
are built entirely on those of Peter and one can even find the appropriate 
references to the Arithmetization of Logic given by Kleene in his Introduction 
to Metamathematics.
It is very frustrating to me to not see more mention of Peter's work on 
Recursive Functions in modern literature.
This is not only because she is a woman, something which people tend to 
overlook or forget to mention, but because before her work on recursive 
functions there was no single collection of what constitute recursive functions 
and how the differing forms of recursion and recursiveness are or are not 
reducible to forms of primitive recursion.

As much as modern computation is about recursion, Peter should be more 
frequently mentioned in modern texts because it is because of her that we have 
most of our fundamental results in the foundation of recursion and 
recursiveness.


20150927T1138 Goodstein Realized Leibniz' Calculus ratiocinator
I am surprised that more was not made of Goodstein's work at the time it was 
completed.
For those familiar with Leibniz's efforts to develop what he referred to as a 
"Calculus Ratiocinator" they will find in Goodstein's 'Recursive Number Theory' 
a complete exposition of the ideal desired by Leibniz.
Sadly, due to the popularity of Leibniz's work, there has been a great number 
of misconceptions as to what was or would become his final aim in creating a 
Calculus ratiocinator.
Some would describe his desired goal as an "algebra of rational thought" or as 
an anticipation of modern mathematical logic.
Leibniz's contributions to modern mathematical logic and philosophy are perhaps 
best encapsulated in Russell's work, though certainly there is no substitute 
for the primary sources.

To answer the question "In what way did Goodstein realize Leibniz's Calculus 
Ratiocinator?".
Goodstein's recursive number theory is a surprisingly simple formalization of 
primitive recursive function theory, but, more importantly, it is an intuitive 
foundation for all of number theory.
Furthermore, he is able to show how what passes as modern mathematical logic is 
developed entirely through the use of arithmetic operations.
It is important to contrast this with the works of Kleene and Gödel.
Kleene and Gödel both developed an arithmetization of logic as a tool for 
reducing their metamathematical arguments to the most "trustworthy" operations 
of arithmetic.
Goodstein began not by seeking out a logical machinery, but rather a self 
contained description of arithmetic as the art of primitive recursive 
reductions and abbreviations.

It is known that Leibniz attempted many times to develop his Calculus 
Ratiocinator using the elementary operations of arithmetic.
One hardly needs a primary source to imagine a man of his intellect trying to 
seek out patterns in arithmetic which matched or met his desired goals in 
research.
Prior to any of our modern mathematical logic was elementary arithmetic, and it 
is in elementary arithmetic that notions of system, structure, and irrefutable 
proof and truth developed.
A modern mathematician would say "Leibniz sought a model of mathematical logic 
in elementary arithmetic" though it took the works of Boole to give a clearer 
hint at where one might find an arithmetic or algebra of argument.

Leibniz would have recognized immediately that Goodstein's formalism of 
primitive recursive arithmetic is precisely the Calculus ratiocinator sought 
for the following reasons:
1 The only inference rules are rules of substitution which encapsulate his 
'identity of indiscernibles' and primitive recursive uniqueness rule which is a 
further application of identity of indiscernibles.
2 All arguments are made by eliminating notation until a numeral is reached.
3 Goodstein's development of mathematical logic in his primitive recursive 
arithmetic satisfies Leibniz's law of identity/contradiction.
4 In Goodstein's system only verifiable equations are provable which I believe 
Leibniz would have quickly interpreted as an interpretation of his law of 
Sufficient Reason.
5 Perhaps the most important thing, Goodstein has produce a clear, complete, 
simple, and general specification of the mechanical movements need by any 
notation which wishes to capture the most basic of mathematical acts: 
elementary arithmetic.

The reasons for believing Goodstein's work realizes Leibniz's ideal go on and 
on.
Anyone familiar with Leibniz should read Goodstein and see what they've missed 
all these years.
I can not overstate the importance of making this connection between the work 
of Leibniz and the work of Goodstein.
It may come as no surprise that my work on N has been inspired and motivated by 
a desire to craft these concepts in the most computable and calculable form 
possible.
Modern mathematics, while a tool of great power and generality, is still a 
poorly designed tool, one whose use requires an almost comical amount of 
expertise to wield wisely.
My purpose in making N and in my work is to refine the design of mathematics so 
that its use may give the widest impact on everyday life which it inevitably 
must have.


20150926T1631 Goodstein's Rt is a number theorists isqrt 
The integer square root of a number n is the largest number whose square is 
less than or equal to n.
It is possible to restrict this to just less than, and since I have not 
considered the boundary cases yet I will go with the convention established by 
number theorists.
The integer square root is fundamental to modern mathematical behavior.
It is also an essential part in the reduction proofs created by Peter in her 
Recursive Functions.
Its importance across all mathematics is not yet widely appreciated because its 
explicit use is often not mentioned.
It's importance is so significant that I have dedicated the unary symbol % of N 
to the isqrt operation.
From now on % will, in my mind, denote the integer square root of its argument 
in unary, and will denote the quotient of x divided by y in the binary case.

20150925T1520 Topics to Think About
Growing Ideas and Understanding with Git and GitHub
Emergence of logic from arithmetic rather than the other way around.
 Logic as a tool for thought is powerful but requires expert experience.
 Arithmetic as a tool for thought is available to all, and as a foundation for 
future understanding has yet to be fully utilized.


20150925T1455 Things I Should Think More About and Do
The design of this website is ugly.
This is primarily because I have put my time and effort into learning and 
understanding topics than I have into presenting what I've learned and 
understood about those topics.
There is a balance that one must strike in the work that they do.
That is a balance between doing work and adequately presenting the work that 
you have done.
When we say "presenting the work we've done" we're actually completing the 
final step of solving a problem: look back.
When we look back at what we've done we see how to change and alter its 
presentation so that whatever solution we've found might be seen at-a-glance or 
at least may be processed by our future self or others with the least amount of 
cognitive effort.

Another reason the design this website is ugly is because you can not design 
around nothing.
Before I spend time on a design I must identify relevant constraints on that 
design.
For this website the primary constraint is the content that I choose to put 
here.
Without clarifying the content I can not begin to clarify a design.
Another way of saying this is that if design has a goal then that goal must be 
identified before we begin the design process.
Here the goal is likely something like "Present arguments and information with 
crystal clarity."
This means that there should be as few moving parts to the mechanism of 
presentation as possible for the arguments and information given here already 
have an over abundance of moving parts.


20150925T1454 
The things people see and feel are guided by their thoughts and impressions for 
better or for worse.


20150924T1655 Dyad # take|project? Dyad , inject|affix|append
These verbs are obviously some of the most important in N regardless of what 
their ultimate implementation will be, the conceptual kernel of each must be so 
unavoidably necessary that one could not imagine living without them.
For # it's operation is projection i.e. it is what a mathematician would most 
likely call a pi-function (not the number pi, the function pi).
It's left argument is a list of indexes and its right is the the object to be 
projected.
Since N supports something like python's tuples, or C++'s homogenous vectors, 
it is easy to see that projection is what is happening if you take the items at 
the left arguments indices.
What's hard to know is if take is the correctly word for humans to use when 
referring to these general acts of projection, especially when dealign with 
tree like objects, dictionary|map like objects, or other objects.
I'm not saying that anything like dictionary|map is unavoidable as an 
object(noun), but that one must consider whether take is an appropriate verb 
for such things over project in case they are ultimately found to be 
unavoidable nouns for achieving N's goals.
x # y when x is a numeral and y is a numeral vector (table?) returns x items 
from y going back to the beginning of y if x exceeds the index of y i.e. the 
k-th item of x # y is the item of y whose index is the remainder of the length 
of y divided by k.
This style of index arithmetic is intended to eliminate references to items 
outside the index of a noun.
Some see this as a huge loss of opportunity to collect common index-errors i.e. 
rather than the whole show stopping just because an index is outside the length 
of a vector object the show just goes on without alerting the coder.
My response to a critic from this perspective is that you must handle your 
errors to know when they occur!
In other words: own your errors, don't blame the computer.
In other other words: you can't reference things outside a noun using clever 
index arithmetic, because the index arithmetic is wiser than you (don't go 
against math, work with math).
Also, a vector is not a list.
Which brings me to , as injection or affix.
injection is different in behavior for affix-ion
an injection usually makes things flat in the relevant way, an affix-ion can be 
seen as either a nesting (tuples of tuples to python people), or perhaps a type 
of injection at different locations.
The use of injection as a descriptor of the behavior of the dyad , is difficult 
to argue from a design perspective (it is not a friendly word to English 
speakers), but affix is also connotative of a potentially misleading 
perspective (that of nesting where no nesting has occurred).
Though the act of injection might not be easily suggested as it requires a left 
argument having a sort of permutation like structure.
For example,  (x;y),z might be interpreted as a command to put z at x in y so 
as to make a new flat vector i.e. to ammend y so as to make room for z.
There is also the potential of using integer notation to give a three command 
form to , so that x 0n2, y gives , an adverbial behavior.
It's not clear if this makes conceptual sense.
These subtle idioms of the language are to be developed as needed, and so far 
affix will probably do fine for the dyad , .

20150924T1602 Hardy, Littlewood, and Pólya Inequalities
I read this book a while back, and happened to be tying my shoes in my closet 
today when my gaze came upon it.
I realized immediately that all the work I've been doing to develop N could be 
put to good use, or to good test, by translating and interpreting the arguments 
and result in N and seeing where they fit inside the arguments given by 
Goodstein in RNT.
Most importantly, HLP consider their work to cover the elementary inequalities 
in use throughout real, complex, and functional analysis.
My perspective on real, complex, and functional analysis is significantly 
different from theirs and most modern mathematicians, so it should be 
interesting to see where I can fit their results in my head.
The good thing is that HLP are smarter and wiser than I am likely to ever be, 
and their results are given in their most fundamental form which is what is of 
direct interest to me and my notational language N.


20150924T1553 Quotes and Goodstein's Introduction of Zero in FCOM
The main thing to remember is to keep the main thing the main thing.
Time stamp everything, you never know when you need to know when.
Well done is better than well said.
If it's meant to be then it's up to me.

Goodstein waits until page 43 to introduce the number zero!
It's brilliant and genius.
He gives zero meaning as a number by introducing it with modular arithmetic, or 
what he more aptly calls 'arithmetic of remainders'.
This is a design feature of Goodstein's perspective and methods.
He doesn't just want to do as much as he can with as little as he can.
He wants to do it in a clean, clear, and vivid way.
A way that humans can follow and that seems to almost be self justifying at 
each step.
Goodstein's Fundamental Concepts of Mathematics is a must read for any math 
teacher at the middle or high school level.
Not because middle or high school teachers will cover the material in it (they 
should, but it's unlikely they would), but because of the vivid perspective it 
gives on the part math plays in everyday life.
It's the only book of its kind that I've ever read (and I've read A LOT of math 
books, like a whole lot, and when I say I've read them, I mean I really went 
through and read every page, something that a lot of people tend not to do, 
especially when the book is really popular).
It's also interesting to note that Goodstein uses the sign 0 for counting with 
an abacus using standard decimal notation, but there it is a place holder, and 
the digit by itself is not yet recognized as a numeral i.e. not clearly 
denoting a thing that we will perform arithmetical acts with.
The reason it makes sense to introduce zero with remainders is because you can 
think of remainder as what is left over after you've taken away as many 
collections containing the dividend number of objects from the collection being 
divided.
It's genius, because it means that zero can be interpreted as what's left over 
after everything has been taken away.
At the same time, by deferring its introduction we avoid a massive number of 
questions about how best to define, via recursion, the basic operations of 
arithmetic: addition, subtraction, multiplication, exponentiation, titration 
etc.
If you want a crystal clear description of the fundamental concepts of modern 
mathematics look no further than Goodstein's "Fundamental Concepts of 
Mathematics".



I want to build a list comprehension using the following code:
[[x,y] if 3!=x+y else [] for x in range(3) for y in range(4)]
but instead of inserting an empty list where the condition is not met, I want 
it to not do anything and just go on.

archive a message from Mail.app with control+command+A

Errors should be tracked across all areas of life.

Learn from your mistakes.
Don't find fault, find a remedy.

Download latest Python (3.5) pre packaged for mac
Invoke from Terminal with python3.5
The version of python invoked with 'python' is 2.7.10

Learning is an endless bootstrapping process.

http://brew.sh/
ruby -e "$(curl -fsSL 
https://raw.githubusercontent.com/Homebrew/install/master/install)"
brew update
brew install gcc
man gcc-5

tree/node notation
a
|\
b c
|\
d c
|
e

a.c
a.b
a.b.c
a.b.d
a.b.d.e
note, a.c ~= a.b.c (unless they match)
trees locally, graphs globally


colophon
John Meuser
Inconsolata
Solarized
Git
GitHub
HTML
CSS
ed
sed
TextEdit

projects
goals
purpose
jsource
music

dictionaries
key-values
hash
Algorithms
Data Structures
Programming Languages
Artificial Intelligence
Databases
Security
Distributed Systems
Operating Systems
Networking
Topologies
Protocols
Applications
Network Congestion
Network Resilience
Popular Operating Systems
Mobile Operating Systems
Special Operating Systems
Components
Client-Server
Map-Reduce
ACID
CAP
Concurrency
Synchronization
Cryptography
Hashing
Information Security
Network Security
Secure Coding
Authentication
Relational Databases
NoSQL Databases
Object-oriented Databases
Database Design and Modeling
Transactions and concurrency
Administration
Storage
Database Security
Machine Learning
Natural Language Processing
Deep Learning
Search &amp; Optimization
Reasoning
Classification
Statistical Learning
Game Theory
Popular Languages
Scripting Languages
Web Languages
Mobile Languages
Functional Languages
Esoteric Languages
Other Languages
Lists
Arrays
Trees
Hashes
Graphs
Sorting
Search
Recursion
Dynamic Programming
Greedy Algorithms
Strings
Graph Theory
Combinatorics
Number Theory
Bit Manipulation
Summations and Algebra
Probability
Geometry
Randomized Algorithms
NP Complete problems
Analytic number theory
Algebraic number theory
Probabilistic number theory
Enumerative combinatorics
Analytic combinatorics
Matroid theory
Probabilistic combinatorics
Algebraic combinatorics
Geometric combinatorics
Topological combinatorics
Arithmetic combinatorics
Combinatorial optimization
Discrete and computational geometry
Elementary Graph Algorithms
Minimum Spanning Trees
Single-Source Shortest Paths
All-Pairs Shortest Paths
Maximum Flow
Floyd-Warshall
Concatenation &amp; Substrings
Prefixes &amp; Suffixes
Rotations
Reversal
Ordering
Encoding
Representation
Parsing
Mining
Sequencing
Partitioning
Searching
Manipulation
Matching
Regular Expressionss
Pure Greedy
Orthogonal
Relaxed
Dijkstra's shortest path algorithm
Fibonacci sequence
Matrix Chain Multiplication
Longest Common Subsequence
Sequence alignment
Top-Down
Bottom-Up
Backtracking
Binary Search
Breadth First Search
Depth First Search
Combinatorial Search
Simple Sorts
Efficient Sorts
Bubble Sorts
Distribution Sorts
Basic Graph
Adjacency list
Adjacency matrix
Binary decision diagram
Directed graph
Directed acyclic graph
Multigraph
Hypergraph
Hash table
Hash list
Hash tree
Hash trie
Bloom filter
Distributed hash table
Double Hashing
Dynamic perfect hash table
Prefix hash tree
Space Partitioning Trees
Application Specific Trees
Binary Trees
B-Trees
Multiway Trees
Heaps
Tries
Bit array
Bit field
Bitmap
Dynamic array
Hashed array tree
Lookup table
Matrix
Parallel array
Sorted array
Sparse matrix
Variable length array
Linked list
Doubly linked list
Array list
Self-organizing list
Skip list
Doubly connected edge list
Difference list
Free list
VB .NET
Pascal
R
D
Groovy
Brainfuck
LOLCODE
WhiteSpace
Scala
Haskell
Clojure
Erlang
F#
OCaml
Racket
Common LISP
SWIFT
Objective C
Php
Javascript
HTML5
Perl
Lua
C
C++
Java
Python
Ruby
C#
Question answering
Sentiment Analysis
Speech Recognition
Text-to-Speech Conversion
Named entity recognition
Decision tree learning
Association rule learning
Artificial neural networks
Inductive logic programming
Support vector machines
Clustering
Bayesian networks
Reinforcement learning
Representation learning
Similarity and metric learning
Sparse dictionary learning
Genetic algorithms
Cassandra
HBase
MongoDB
DB2
PostgreSQL
Microsoft SQL Server
MySQL
Symmetric-key cryptography
Public-key cryptography
Cryptanalysis
Cryptographic primitives
Cryptosystems
Kernel
File Systems
Memory Management
Process Management
Distributed Operating Systems
Network Operating Systems
Object Oriented Operating Systems
Embedded Operating Systems
Android
iOS
Windows Phone
Linux
OSX
BSD
UNIX
Windows
IPTV
Videoconferencing
Online games
VoIP
Routing Protocols
Secure Protocols
TCP
IP
HTTP
POP
IMAP
FTP
Basic Trie
Radix tree
Suffix tree
Suffix array
B-trie
Binary heap
Weak heap
Binomial heap
Fibonacci heap
Ternary tree
Disjoint-set
Fusion tree
Fenwick tree
Basic B-tree
B+ tree
B* tree
2-3 tree
2-3-4 tree
Queap
Binary Search Tree
Self balancing Tree
Red-Black Trees
AVL tree
Splay tree
Weight-balanced tree
Abstract syntax tree
Parse tree
Decision tree
Minimax tree
Segment tree
R-tree
Counting sort
Bucket sort
Radix sort
Bubble sort
Shell sort
Comb sort
Mergesort
Heapsort
Quicksort
Insertion sort
Selection sort
Naive String Matching
Rabin-Karp
Finite Automata
Naive string search
Finite State Automaton
Index
Fuzzy Searches
Breadth First Search
Depth First Search
Dijkstra's Shortest Path
</pre></body></html>
